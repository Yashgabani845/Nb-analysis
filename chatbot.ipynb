{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVuYdVMO0t1RR57Aq9uh8S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yashgabani845/Nb-analysis/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cPCwqMZokDLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy"
      ],
      "metadata": {
        "id": "ydD14uf1kHOB"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's start step by as we know in our transformer mechanism first thing that we do with input is token embedding and convert strung into number or specifically array on number"
      ],
      "metadata": {
        "id": "ilnMlLs42Yvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# nn.Module says it is layer of neural Network\n",
        "#d_model = embedding size\n",
        "class TokenEmbedding(nn.Module):\n",
        "  def __init__(self,vocb_size,d_model):\n",
        "    super().__init__();\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    #create embeddign  layer\n",
        "  def forward(self , x):\n",
        "      #x will have vector which is embedded that we pass in next layer\n",
        "      return self.embedding(x)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        # Create a positional encoding matrix\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Add batch dimension\n",
        "        pe = pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
        "\n",
        "        # Register buffer so it is not a trainable parameter\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "      \"\"\"\n",
        "      x: Tensor of shape (batch_size, seq_length, d_model)\n",
        "      Returns: Tensor of same shape with positional encoding added\n",
        "      \"\"\"\n",
        "      #print(\"Input tensor shape:\", x.shape)  # Debug print\n",
        "      seq_length = x.size(1)\n",
        "      #print(\"Positional encoding shape:\", self.pe[:, :seq_length, :].shape)  # Debug print\n",
        "      # Expand the positional encoding to match the batch size\n",
        "      # this will replicate your positional encoding along the 0th dimension and make the size of positional_encoding = (4,20,512)\n",
        "      positional_encoding = self.pe[:, :seq_length, :].expand(x.size(0), -1, -1)\n",
        "\n",
        "      return x + positional_encoding # Now the size of 'x' and positional encoding is the same and we will be able to add them"
      ],
      "metadata": {
        "id": "3eSWdN3D2okR"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledProductAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "     super().__init__()\n",
        "\n",
        "  def forward(self,query , key , value , mask = None):\n",
        "    d_k = query.size(-1)\n",
        "    scores = (query @ key.transpose(-2,-1))/np.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "      scores = scores.masked_fill(mask==0,-1e9)\n",
        "\n",
        "\n",
        "    attention = torch.softmax(scores,dim=-1) # This line was indented inside the if statement, causing the error when mask was None.\n",
        "    return attention @ value # This line was indented inside the if statement, causing the error when mask was None."
      ],
      "metadata": {
        "id": "QmPB5YmV3DTg"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,heads,d_model):\n",
        "    super().__init__()\n",
        "    self.heads = heads\n",
        "    self.d_k = d_model // heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.W_query = nn.Linear(d_model , d_model)\n",
        "    self.W_key = nn.Linear(d_model , d_model)\n",
        "    self.W_value = nn.Linear(d_model , d_model)\n",
        "    self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    self.attention = ScaledProductAttention()\n",
        "\n",
        "\n",
        "  def forward(self, query, key, value, mask=None):\n",
        "    batch_size = query.shape[0]\n",
        "\n",
        "    # Apply linear transformations and reshape\n",
        "    query = self.W_query(query).view(batch_size, -1, self.heads, self.d_k).transpose(1, 2)\n",
        "    key = self.W_key(key).view(batch_size, -1, self.heads, self.d_k).transpose(1, 2)\n",
        "    value = self.W_value(value).view(batch_size, -1, self.heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    # Calculate attention\n",
        "    atten_output = self.attention(query, key, value, mask)\n",
        "\n",
        "    # Concatenate and reshape\n",
        "    concat_output = atten_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "\n",
        "    # Apply final linear transformation\n",
        "    return self.W_o(concat_output)"
      ],
      "metadata": {
        "id": "168IJ5T6MkTy"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNetwork(nn.Module):\n",
        "  def __init__(self, d_model, d_ff):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(d_model, d_ff)\n",
        "    self.fc2 = nn.Linear(d_ff, d_model)\n",
        "    self.relu= nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.fc2(self.relu(self.fc1(x)))"
      ],
      "metadata": {
        "id": "PhAujq-aN-bR"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self,d_model,heads,d_ff):\n",
        "    super().__init__()\n",
        "    self.attention = MultiHeadAttention(heads,d_model)\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.ffn = FeedForwardNetwork(d_model,d_ff)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self , x, mask=None):\n",
        "      atten_out = self.attention(x,x,x,mask)\n",
        "      x = self.norm1(x+atten_out)\n",
        "      ffn_out = self.ffn(x)\n",
        "      return self.norm2(x+ffn_out)\n",
        "      return x"
      ],
      "metadata": {
        "id": "PnoNi3n8OIIa"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, heads, d_ff):\n",
        "    super().__init__()\n",
        "    self.attention = MultiHeadAttention(heads, d_model)\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    # Pass heads and d_model in the correct order\n",
        "    self.enc_dec_attention = MultiHeadAttention(heads, d_model)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "    self.ffn = FeedForwardNetwork(d_model, d_ff)\n",
        "    self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x, encoder_output, src_mask=None, target_mask=None):\n",
        "    x = self.norm1(x + self.attention(x, x, x, target_mask))\n",
        "    x = self.norm2(x + self.enc_dec_attention(x, encoder_output, encoder_output, src_mask))\n",
        "    #call ffn instead of fnn\n",
        "    x = self.norm3(x + self.ffn(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "fac79_VaOlDr"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__ (self , vocab_size, d_model , heads , d_ff , num_layers):\n",
        "    super().__init__()\n",
        "    self.embedding = TokenEmbedding(vocab_size,d_model)\n",
        "    self.pe = PositionalEncoding(d_model)\n",
        "    self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, heads, d_ff) for _ in range(num_layers)])\n",
        "    self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, heads, d_ff) for _ in range(num_layers)])\n",
        "\n",
        "  def forward(self, src, tgt):\n",
        "    src = self.pe(self.embedding(src))\n",
        "    tgt = self.pe(self.embedding(tgt))\n",
        "    for layer in self.encoder_layers:\n",
        "      src= layer(src)\n",
        "    for layer in self.decoder_layers:\n",
        "      tgt = layer(tgt,src)\n",
        "    return tgt"
      ],
      "metadata": {
        "id": "initUt9tP26A"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Example dataset: 10 sentences with 20 tokens each\n",
        "num_sentences = 10\n",
        "seq_length = 20\n",
        "batch_size = 4\n",
        "vocab_size = 10000\n",
        "\n",
        "# Generate dummy dataset (Replace with real tokenized data)\n",
        "src_data = torch.randint(0, vocab_size, (num_sentences, seq_length))\n",
        "tgt_data = torch.randint(0, vocab_size, (num_sentences, seq_length))\n",
        "\n",
        "# Create DataLoader for batching\n",
        "dataset = TensorDataset(src_data, tgt_data)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define Transformer Model\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, heads, d_ff, num_layers):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoding(d_model)\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, heads, d_ff) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, heads, d_ff) for _ in range(num_layers)])\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)  # Final projection layer\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src = self.pe(self.embedding(src))\n",
        "        tgt = self.pe(self.embedding(tgt))\n",
        "\n",
        "        # Encoder Pass\n",
        "        for layer in self.encoder_layers:\n",
        "            src = layer(src)\n",
        "\n",
        "        # Decoder Pass\n",
        "        for layer in self.decoder_layers:\n",
        "            tgt = layer(tgt, src)\n",
        "\n",
        "        return self.fc_out(tgt)\n",
        "\n",
        "# Instantiate Model\n",
        "model = Transformer(vocab_size, d_model=512, heads=8, d_ff=2048, num_layers=6)\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00005)\n",
        "\n",
        "# Training Loop with DataLoader\n",
        "model.train()\n",
        "for epoch in range(5):\n",
        "    for src, tgt in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, tgt)  # Forward pass\n",
        "        output = output.view(-1, vocab_size)  # Flatten for loss computation\n",
        "        tgt = tgt.view(-1)  # Flatten target\n",
        "\n",
        "        loss = criterion(output, tgt)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/5, Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKfucP4OQT-8",
        "outputId": "f8b4b95b-49a3-4b7b-d2a8-554b2166e47b"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 9.37251091003418\n",
            "Epoch 2/5, Loss: 8.217771530151367\n",
            "Epoch 3/5, Loss: 8.025714874267578\n",
            "Epoch 4/5, Loss: 7.621241092681885\n",
            "Epoch 5/5, Loss: 7.54739236831665\n"
          ]
        }
      ]
    }
  ]
}